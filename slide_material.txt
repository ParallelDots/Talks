Slide 1:

ParallelDots is an Artificial Intelligence company which makes innovative products for developers and enterprises.
Deep Learning algorithms as a service, where we train our algorithms on user datasets and help them power their apps using these algorithms as an API.

We have developed the following algorithms:
1.Semantic Proximity
2.Entity Extraction
3.Taxonomy
4.Sentiment Analysis.

So today's talk will be about how we have built a contextual recommendation engine using a combination of these technologies. This contextual posts plugin includes a related posts plugin, a timeline and soon to come tagclouds.


Slide 2:

There are numerous related post plugins already, but they fail at scale. The recommendations are often garbage and sometimes not visible at all. This happens due to their TFIDF based search algo and their very generic SEO tags.

Aim of this recommendation engine is to build a solution that is more accurate, cheaper and easier to deploy.

Slide 3:

We made our MVP with Topic Modelling algortihms like LSA, incremental LDA, but they were not accurate enough at due to high grandularity in data and were never scalable to begin with. Hence we shifted to this Deep Learning based approach.


So, our current solution works in the follwoing way:

1. First we train Word Embeddings for all the words. Word Embeddings are dense low dimensional representation for each word. You can think of the word embedidngs as thought vectors. So, think of it this way - say you have a thought vecotr/representation of following words 'king', 'queen' and 'man'. Now if you subtract, the thought vector of 'man' from that of the word 'king' you will ideally reach near the though vector of the word 'queen'.
2. Now we combine thse word thought vectors into phrase/document thought vectors and there are two different ways to approach this. If we want documents to be in a hyperspace where documents with common entities are close, we use some heuristics over the word thought vectors. When we want the combined vector to have close semantic similarity, we use a Recursive Neural Network to combine them. Depending on the necessity we choose our approach.
3. Now that we have a representation for each document, we need to search the closest document to an incoming one as soon as possible. One approach was to use dostributed computing for this, but as it turns out, the costs involved were high. SO we wrote an efficient implememtation of a space partitioning tree algorithm to search for nearest neighbors.
4. The last but not not the least problem was the insane spikes on these high traffic news websites. Before you can get related posts for an article and cache it, the concurrent traffic would overwhelm you. SO we needed a customised pub-sub mechanism, which could deduplicate these requests and reduce load on machine learning server. We accomplished this using channels in GoLang.



Slide 4 (Word Embeddings):

1. There are some Open Sourced implementations of WOrd Embeddings. The most popular is Word2Vec by Google. Google recently patented it, and despite its good record in patents, it makes people anxious. We have hence tried to come up with our own implementation of word embeddings and open sourced it. 
It is basically an autoencoder which tries to reconstruct the co-occurance matrix.
Our implementation requires comparatively lower resources like RAM and can be trained even on a slow CPU. This means we can use it on very large datasets with ease.
2. We are furthur evolving to use character level models where we wont even need word embeddings.


Slide 5 (Deep Learning Models we use):

As I mentioned earlier we have 4 different Deep Learning Models. In this search engine we use Recursive Neural Network based semantic similarity and taxonomy or some heuristics.

The Recursive Neural Network we use minimizes reconstruction cost at various nodes of the tree of each phrase, where it tries to come up with a set of weights (or you can say rules) to make tree for all phrases.

In case we are interested to get aggregations of word embddings where entities are given more priority than semantic meaning , we use some heuristics to combine them.

There is a recursive neural network which looks in a neighborhood of Word Embeddings to find named entities in it.

There are other models that we have released as APIs but do not use in the recommendation engine. These include a convolutional Neural net which convolves N-Grams of Word Embeddings to predict sentiment.

 

Slide 6(VP Tree):

Now about Space Patitioning Trees. The logic is to recursively partition the search space to reach to an area where all relevant points are located. Building such a tree with documents drastically reduces the number of distance calculations. We developed our own fast implementation of Vantage Point Tree in NumPy to use in system.

Right now we parallelize it but dividing data into multiple buckets and loading VP Tree on each bucket onto one core and then parallely search on these VP Trees using Python's multiprocessing. This makes our search become almost O(log N). IN future we plan to make a shared memory based parallel implementation which would make our algorithm faster that is true O(log N).

Slide 7 (Web Service):

FInally this is about how we handle the insane traffic of news websites. We are deploying these solutions on publishers who serve hundred of millions of impressions per month and there articles go viral as soon as they put it onsocial media. so our MVP which was aimed at caching recommendation for articles and serving them again and again was overwhelmed. 
We found out that the traffic of news websites is huge, but they are directed to a certain set of unique articles. Now we needed a method to recognize what are these unique articles and generate recommendations only for them. We wrote a system using Golang's channels to do this concurrently while we get re	quests for rcommendations.

Slide 8 (Basics of Deep Learning):

So now I will talk about basics of Deep Learning and how we implement the algorithms I mentioned before this. Deep Learning is a multiple layered Neural net, where each layer is separated by an activation function.






 



