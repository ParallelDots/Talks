Slide 1:

ParallelDots is an Artificial Intelligence company which makes innovative products for developers and enterprises.
Deep Learning algorithms as a service, where we train our algorithms on user datasets and help them power their apps using these algorithms as an API.

We have developed the following algorithms:
1.Semantic Proximity
2.Entity Extraction
3.Taxonomy
4.Sentiment Analysis.

So today's talk will be about how we have built a contextual recommendation engine using a combination of these technologies. This contextual posts plugin includes a related posts plugin, a timeline and soon to come tagclouds.


Slide 2:

There are numerous related post plugins already, but they fail at scale. The recommendations are often garbage and sometimes not visible at all. This happens due to their TFIDF based search algo and their very generic SEO tags.

Aim of this recommendation engine is to build a solution that is more accurate, cheaper and easier to deploy.

Slide 3:

We made our MVP with Topic Modelling algortihms like LSA, incremental LDA, but they were not accurate enough at due to high grandularity in data and were never scalable to begin with. Hence we shifted to this Deep Learning based approach.


So, our current solution works in the follwoing way:

1. First we train Word Embeddings for all the words. Word Embeddings are dense low dimensional representation for each word. You can think of the word embedidngs as thought vectors. So, think of it this way - say you have a thought vecotr/representation of following words 'king', 'queen' and 'man'. Now if you subtract, the thought vector of 'man' from that of the word 'king' you will ideally reach near the though vector of the word 'queen'.
2. Now we combine thse word thought vectors into phrase/document thought vectors and there are two different ways to approach this. If we want documents to be in a hyperspace where documents with common entities are close, we use some heuristics over the word thought vectors. When we want the combined vector to have close semantic similarity, we use a Recursive Neural Network to combine them. Depending on the necessity we choose our approach.
3. Now that we have a representation for each document, we need to search the closest document to an incoming one as soon as possible. One approach was to use dostributed computing for this, but as it turns out, the costs involved were high. SO we wrote an efficient implememtation of a space partitioning tree algorithm to search for nearest neighbors.
4. The last but not not the least problem was the insane spikes on these high traffic news websites. Before you can get related posts for an article and cache it, the concurrent traffic would overwhelm you. SO we needed a customised pub-sub mechanism, which could deduplicate these requests and reduce load on machine learning server. We accomplished this using channels in GoLang.



Slide 4 (Word Embeddings):

1. There are some Open Sourced implementations of WOrd Embeddings. The most popular is Word2Vec by Google. But to everyone's surprise - Google just recently patented it. It got us worried and in that state of mind we ended up developing our own implementation of word embeddings. We have open sourced it and people can start using it from our Github page. 
It is basically an autoencoder which tries to reconstruct the co-occurance matrix.
The main advantage of our implementation of word embeddings is the it requires much lower resources like RAM and can be trained even on a slow CPU. This means we can use it on very large datasets with ease.
2. We are furthur evolving to use character level models where we wont even need these word embeddings.


Slide 5 (Deep Learning Models we use):

As I mentioned earlier we have 4 different Deep Learning technologies. To find related context we either use Recursive Neural Network or some heuristics depending on the necessity.

The Recursive Neural Network we use minimizes reconstruction cost at various nodes of the tree of each phrase, where it tries to come up with a set of weights (or you can say rules) to make tree for all phrases.

In case we are interested to get aggregations of word embeddings where entities are given more priority than semantic meaning , we use some heuristics to combine them.

But before we use heuristics, there's another different recursive neural network which looks in a neighborhood of Word Embeddings to find named entities in it.

There are other models that we have released as APIs but do not use in the recommendation engine. These include a convolutional Neural net which convolves N-Grams of Word Embeddings to predict sentiment.

 

Slide 6(VP Tree):

Now about Space Patitioning Trees. The logic is to recursively partition the search space to reach to an area where all relevant points are located. So, we again developed our own fast implementation of Vantage Point Tree in NumPy to use in system. Our VP tree of documents drastically reduces the number of distance calculations making it much faster.

Right now we parallelize it by dividing data into multiple buckets and loading VP Tree on each bucket onto one core and then parallely search on these VP Trees using Python's multiprocessing. This makes our search become almost O(log N). IN future we plan to make a shared memory based parallel implementation which would make our algorithm faster that is true O(log N).

Slide 7 (Web Service):

FInally coming to how we handle the insane traffic of news websites. We are deploying these solutions on publishers who serve hundreds of millions of impressions per month and there articles go viral as soon as they are shared on social media. So our simple MVP which was aimed at caching recommendation for articles and serving them again and again was overwhelmed by huge spikes in traffic. 
We realized that this traffic is mostly directed to a certain set of unique articles. Now we needed a method to recognize what are these unique articles and generate recommendations only for them. We wrote a system using Golang's channels to do this concurrently while we get requests for rcommendations. So now we can easily handle 5000 concurrent users on a very small machine.

Slide 8 (Basics of Deep Learning):

This was more or less everything about our technology. I'd now like to walk you through the very basics of Deep Learning and how do you actually implement these algorithms. Deep Learning is a multiple layered Neural net, where each layer is separated by an activation function.

Slide 9 (Types of Neural Networks) :
Convolutional Neural Networks try to model human visual cortex. They try to optimize various layers of Convolution Matrices over images/text to extract higher level of abstractions. So these networks can extract abstract features from media, like human eyes in an image.

RBMs (and DBNs) try to model probabilistic graphical models as neural networks. 

Recurrent Neural network try to model arbitrary length chains and Recursive Neural Nets try to model tree structure. Recurrent Neural Networks achieve this by using many cool neural units. Recursive Neural Networks recursively apply same weight multiple times to convert a sequence into a tree.


Slide 9 (Vocab):

Here are some more terms related to neural nets. they can be divided into some type of architecture, neural unit or optimization method. If there was more time, I would have discussed each in detail.

Slide 10(Implementations):